{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a969d6df01744ec7aeff653f56ca38d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17f530c81a2747afa775a79d3f6d58ec",
              "IPY_MODEL_d77d04bdaeaa4b3abd33aa176744505b",
              "IPY_MODEL_358ffc086477427291923933e9b00a26"
            ],
            "layout": "IPY_MODEL_4a29ba17f90d43ce8bd6a2ecd9d45330"
          }
        },
        "17f530c81a2747afa775a79d3f6d58ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_630f81fc0a434c62b94bb6b6b8ae66d5",
            "placeholder": "​",
            "style": "IPY_MODEL_ee2171fab9304616a812feeeadce14de",
            "value": "Loading weights: 100%"
          }
        },
        "d77d04bdaeaa4b3abd33aa176744505b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b46f3c6547f47efa19f61f15d497993",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd9577ec0bff4a19ab7e8051edd80664",
            "value": 148
          }
        },
        "358ffc086477427291923933e9b00a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d7a997e2a6748a69ddb669c2b50aa86",
            "placeholder": "​",
            "style": "IPY_MODEL_bcd4b390bd604d8081bd21a640fc74d9",
            "value": " 148/148 [00:00&lt;00:00, 478.42it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "4a29ba17f90d43ce8bd6a2ecd9d45330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630f81fc0a434c62b94bb6b6b8ae66d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee2171fab9304616a812feeeadce14de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b46f3c6547f47efa19f61f15d497993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9577ec0bff4a19ab7e8051edd80664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d7a997e2a6748a69ddb669c2b50aa86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd4b390bd604d8081bd21a640fc74d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efec5766714545ca9ebc101abf7dfde9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_378f7e73b1734ea38b786aa81738f2cb",
              "IPY_MODEL_7e45f86f7350420cb79798ff05aa5e30",
              "IPY_MODEL_9b97a05ae973465187d2e596c8712a70"
            ],
            "layout": "IPY_MODEL_c084c4fd57d74689b8e14c3a8a18f490"
          }
        },
        "378f7e73b1734ea38b786aa81738f2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38d53e1716fd41548ab75a42bd5e3f9d",
            "placeholder": "​",
            "style": "IPY_MODEL_e8c76549952e40848708e6773281ef59",
            "value": "Loading weights: 100%"
          }
        },
        "7e45f86f7350420cb79798ff05aa5e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d24ad85ac4497781ff8352f4cfce38",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_439ea9664fe94a26aa3f69b543e30577",
            "value": 148
          }
        },
        "9b97a05ae973465187d2e596c8712a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b1939949e0343cba4995b0f200d2def",
            "placeholder": "​",
            "style": "IPY_MODEL_208a0cd5d1634edca2c44da097ca70da",
            "value": " 148/148 [00:00&lt;00:00, 215.09it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "c084c4fd57d74689b8e14c3a8a18f490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d53e1716fd41548ab75a42bd5e3f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8c76549952e40848708e6773281ef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9d24ad85ac4497781ff8352f4cfce38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "439ea9664fe94a26aa3f69b543e30577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b1939949e0343cba4995b0f200d2def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "208a0cd5d1634edca2c44da097ca70da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n"
      ],
      "metadata": {
        "id": "2_sxJbGfUU7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1.  What is Generative AI and what are its primary use cases across\n",
        "industries?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Generative AI refers to AI systems that create new content, such as text, images, audio, video, or code, based on patterns learned from vast datasets. Unlike traditional AI, which analyzes or classifies data, generative models produce original outputs mimicking human creativity.\n",
        "\n",
        "**Core Mechanism :**\n",
        "\n",
        "Generative AI primarily relies on models like GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), and transformers (e.g., GPT architectures). These systems generate content by predicting probable sequences or structures from training data, enabling applications from writing assistants to image synthesis.\n",
        "\n",
        "**Key Use Cases :**\n",
        "\n",
        "1. **Healthcare :**\n",
        "It accelerates drug discovery by proposing novel molecular structures and enhances medical imaging through synthetic data generation. Personalized treatment plans and predictive disease modeling are also common.\n",
        "\n",
        "2. **Manufacturing :**\n",
        "Engineers use it for generative design, optimizing parts for weight, strength, and material efficiency—e.g., General Motors designs lighter vehicle components. Predictive maintenance and quality control reduce downtime and waste.\n",
        "\n",
        "3. **Marketing and Retail :**\n",
        "Tools create personalized content, product recommendations, and marketing copy. Chatbots deliver tailored customer responses, while demand forecasting improves inventory management.\n",
        "\n",
        "4. **Finance and Insurance :**\n",
        "It aids fraud detection, generates financial reports, and simulates scenarios for risk assessment. Personalized strategies enhance customer engagement.\n",
        "\n",
        "5. **Entertainment and Media :**\n",
        "Generative AI produces art, music, scripts, and videos, speeding up creative workflows. It also supports personalized recommendations.\n",
        "\n",
        "6. **Software Development :**\n",
        "Code generation tools like assistants write, debug, and optimize software, reducing development time for less experienced programmers."
      ],
      "metadata": {
        "id": "X09AUOKzUU3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. Explain the role of probabilistic modeling in generative models. How do\n",
        "these models differ from discriminative models?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Probabilistic modeling forms the foundation of generative models by capturing the underlying probability distributions of data, enabling them to generate new samples that resemble the training data. These models learn the joint probability P(X) or P(X,Y) over input features X (and labels Y, if supervised), often factorized as P(X)=∏\n",
        "i=1\n",
        "n\n",
        " P(x\n",
        "i\n",
        " ∣x\n",
        "1\n",
        " ,…,x\n",
        "i−1\n",
        " )  allowing sequential sampling to produce novel outputs like text or images.\n",
        "\n",
        "**Role in Generative Models :**\n",
        "\n",
        "Generative models use probabilistic approaches to estimate data distributions explicitly. For instance, Variational Autoencoders (VAEs) impose probabilistic priors (e.g., Gaussian) on latent spaces and optimize losses like reconstruction error plus KL-divergence to match learned distributions to priors. This enables sampling from the latent space to create diverse, realistic data points, powering applications like image synthesis.\n",
        "​\n",
        "\n",
        "**Differences from Discriminative Models :**\n",
        "\n",
        "- Generative models learn the full data distribution to generate new instances, while discriminative models focus on boundaries between classes by estimating\n",
        "P\n",
        "(\n",
        "Y\n",
        "∣\n",
        "X\n",
        ").\n",
        "- Generative approaches support tasks like data augmentation and anomaly detection; discriminative ones excel in classification but cannot generate data."
      ],
      "metadata": {
        "id": "3KFAmVF-UU0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. What is the difference between Autoencoders and Variational\n",
        "Autoencoders (VAEs) in the context of text generation?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Autoencoders (AEs) and Variational Autoencoders (VAEs) both compress and reconstruct data via encoder-decoder architectures, but VAEs introduce probabilistic elements that make them suitable for generative tasks like text generation.\n",
        "\n",
        "**The difference between Autoencoders and Variational\n",
        "Autoencoders :**\n",
        "\n",
        "Autoencoders :      \n",
        "- In this, latent output is fixed vector.\n",
        "- In this, the loss function is only Reconstructino.\n",
        "- In this, Generative ability is poor (no smooth smapling).\n",
        "- Example : Denoising noisy sentences\n",
        "\n",
        "Variational Autoencoders :     \n",
        "- In this, latent output is distribution parameters.\n",
        "- In this, the loss function is Reconstruction and KL divergence.\n",
        "- In this, Generative ability is strong (probabilistic smapling).\n",
        "- Example : Generating variations of input text."
      ],
      "metadata": {
        "id": "0CpvSHLeUUxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4.  Describe the working of attention mechanisms in Neural Machine\n",
        "Translation (NMT). Why are they critical?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Attention mechanisms in Neural Machine Translation (NMT) enable the decoder to dynamically focus on relevant parts of the source sequence during target generation, overcoming limitations of fixed context vectors.\n",
        "\n",
        "**How Attention Works :**\n",
        "\n",
        "In NMT's encoder-decoder framework (often RNN- or LSTM-based), the encoder processes the source sentence into hidden states h1,h2,…,hn, each representing contextual info at position i. For each decoder timestep t(generating target word yt), attention computes:\n",
        "\n",
        "1. Alignment scores: A similarity function (e.g., dot product) between decoder state st and each encoder state hi:eti =\n",
        "score(st,hi).\n",
        "\n",
        "2. Attention weights: Softmax-normalized scores αti = exp(eti) / ∑j exp(etj), indicating focus on source positions.\n",
        "\n",
        "3. Context vector: Weighted sum ct = ∑i αti hi, fed to the decoder alongside st to predict yt.\n",
        "\n",
        "This repeats per target word, with visualizations showing peak weights aligning source-target pairs (e.g., high focus on \"cat\" when translating a pronoun referring to it).\n",
        "\n",
        "**Why Critical ?**\n",
        "\n",
        "Without attention, decoders rely on a single fixed-length context vector from the final encoder state, causing \"bottleneck\" information loss for long sentences—degrading BLEU scores by up to 5+ points. Attention handles variable-length inputs, captures long-range dependencies, and improves fluency/accuracy, as proven in foundational works yielding major gains on WMT tasks."
      ],
      "metadata": {
        "id": "P9NiSgSFUUty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. What ethical considerations must be addressed when using generative AI\n",
        "for creative content such as poetry or storytelling?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Generative AI for creative content like poetry or storytelling raises ethical challenges around authenticity, ownership, and societal impact. Key considerations include protecting intellectual property, ensuring transparency, and mitigating biases to avoid harm.\n",
        "\n",
        "**Intellectual Property Rights :**\n",
        "AI models trained on vast datasets often reproduce styles or phrases from existing works, risking unintentional plagiarism or copyright infringement. Creators must verify originality, attribute influences when possible, and clarify AI involvement to prevent misrepresenting machine output as human creation.\n",
        "\n",
        "**Transparency and Disclosure :**\n",
        "Failing to disclose AI use deceives audiences, eroding trust in creative fields where human authorship holds value. Ethical practice requires labeling AI-generated poetry or stories, allowing consumers to contextualize the work and distinguish it from human efforts.\n",
        "\n",
        "**Bias and Fairness :**\n",
        "Training data embeds cultural, gender, or racial biases, leading to stereotypical narratives (e.g., clichéd portrayals in storytelling). Developers and users should audit outputs for inclusivity, diversify datasets, and implement fairness checks to promote equitable representation.\n",
        "\n",
        "**Misinformation and Authenticity :**\n",
        "AI can generate convincingly false historical details or emotional manipulations in stories, blurring reality-fiction lines. Responsible use demands fact-checking, especially for poetry evoking real events, and prioritizing human oversight to maintain artistic integrity."
      ],
      "metadata": {
        "id": "ijlfg30PaoG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. Use the following small text dataset to train a simple Variational\n",
        "Autoencoder (VAE) for text reconstruction:\n",
        "\n",
        "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
        "\"The night is dark\", \"The stars are shining\"]\n",
        "\n",
        "1. Preprocess the data (tokenize and pad the sequences).\n",
        "2. Build a basic VAE model for text reconstruction.\n",
        "3. Train the model and show how it reconstructs or generates similar sentences.\n",
        "\n",
        "Include your code, explanation, and sample outputs.\n"
      ],
      "metadata": {
        "id": "QC0ZZIXXa_Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Dataset\n",
        "sentences = [\n",
        "    \"The sky is blue\",\n",
        "    \"The sun is bright\",\n",
        "    \"The grass is green\",\n",
        "    \"The night is dark\",\n",
        "    \"The stars are shining\"\n",
        "]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Padding\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# One-hot encoding for reconstruction\n",
        "one_hot_targets = to_categorical(padded_sequences, num_classes=vocab_size)\n",
        "\n",
        "print(\"Vocabulary:\", word_index)\n",
        "print(\"Padded sequences:\\n\", padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfGhs-_ZbYIV",
        "outputId": "21ecc6c4-c525-4839-f7e9-33807df2351f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'the': 1, 'is': 2, 'sky': 3, 'blue': 4, 'sun': 5, 'bright': 6, 'grass': 7, 'green': 8, 'night': 9, 'dark': 10, 'stars': 11, 'are': 12, 'shining': 13}\n",
            "Padded sequences:\n",
            " [[ 1  3  2  4]\n",
            " [ 1  5  2  6]\n",
            " [ 1  7  2  8]\n",
            " [ 1  9  2 10]\n",
            " [ 1 11 12 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short\n",
        "English paragraph into French and German. Provide the original and translated text."
      ],
      "metadata": {
        "id": "_X6zYT0gbken"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "text = \"\"\"\n",
        "Translate the following paragraph into French:\n",
        "\n",
        "Artificial Intelligence is transforming the world rapidly.\n",
        "It is used in healthcare, finance, education, and transportation\n",
        "to improve efficiency and decision-making.\n",
        "\"\"\"\n",
        "\n",
        "output = generator(text, max_length=200, temperature=0.3)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781,
          "referenced_widgets": [
            "a969d6df01744ec7aeff653f56ca38d6",
            "17f530c81a2747afa775a79d3f6d58ec",
            "d77d04bdaeaa4b3abd33aa176744505b",
            "358ffc086477427291923933e9b00a26",
            "4a29ba17f90d43ce8bd6a2ecd9d45330",
            "630f81fc0a434c62b94bb6b6b8ae66d5",
            "ee2171fab9304616a812feeeadce14de",
            "2b46f3c6547f47efa19f61f15d497993",
            "fd9577ec0bff4a19ab7e8051edd80664",
            "1d7a997e2a6748a69ddb669c2b50aa86",
            "bcd4b390bd604d8081bd21a640fc74d9"
          ]
        },
        "id": "QQ14NRacb59v",
        "outputId": "c3ee7d87-02a2-4de2-c88f-9fb2b667dfea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a969d6df01744ec7aeff653f56ca38d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translate the following paragraph into French:\n",
            "\n",
            "Artificial Intelligence is transforming the world rapidly. \n",
            "It is used in healthcare, finance, education, and transportation \n",
            "to improve efficiency and decision-making.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used in the production of new technologies, including robotics, artificial intelligence, and robotics.\n",
            "\n",
            "It is used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. Implement a simple attention-based encoder-decoder model for\n",
        "English-to-Spanish translation using Tensorflow or PyTorch.\n"
      ],
      "metadata": {
        "id": "8FcGsL1RcEAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer  ->>\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# =========================\n",
        "# 1. Dataset\n",
        "# =========================\n",
        "pairs = [\n",
        "    (\"hello\", \"hola\"),\n",
        "    (\"how are you\", \"como estas\"),\n",
        "    (\"i am fine\", \"estoy bien\"),\n",
        "    (\"thank you\", \"gracias\"),\n",
        "    (\"good night\", \"buenas noches\")\n",
        "]\n",
        "\n",
        "eng_sentences = [p[0] for p in pairs]\n",
        "spa_sentences = [\"<start> \" + p[1] + \" <end>\" for p in pairs]\n",
        "\n",
        "# =========================\n",
        "# 2. Tokenization\n",
        "# =========================\n",
        "eng_tokenizer = Tokenizer(filters='')\n",
        "spa_tokenizer = Tokenizer(filters='')\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "spa_tokenizer.fit_on_texts(spa_sentences)\n",
        "\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "spa_seq = spa_tokenizer.texts_to_sequences(spa_sentences)\n",
        "\n",
        "max_eng_len = max(len(seq) for seq in eng_seq)\n",
        "max_spa_len = max(len(seq) for seq in spa_seq)\n",
        "\n",
        "eng_seq = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
        "spa_seq = pad_sequences(spa_seq, maxlen=max_spa_len, padding='post')\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
        "\n",
        "# =========================\n",
        "# 3. Attention Layer\n",
        "# =========================\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, hidden, encoder_outputs):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)\n",
        "        ))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * encoder_outputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# =========================\n",
        "# 4. Encoder\n",
        "# =========================\n",
        "embedding_dim = 64\n",
        "units = 128\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "# =========================\n",
        "# 5. Decoder\n",
        "# =========================\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(units)\n",
        "\n",
        "    def call(self, x, hidden, cell, encoder_outputs):\n",
        "        context_vector, attention_weights = self.attention(hidden, encoder_outputs)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, cell])\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state_h, state_c, attention_weights\n",
        "\n",
        "# =========================\n",
        "# 6. Initialize Models\n",
        "# =========================\n",
        "encoder = Encoder(eng_vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(spa_vocab_size, embedding_dim, units)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 7. Training Step\n",
        "# =========================\n",
        "@tf.function\n",
        "def train_step(inp, targ):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden, enc_cell = encoder(inp)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_cell = enc_cell\n",
        "\n",
        "        dec_input = tf.expand_dims(\n",
        "            [spa_tokenizer.word_index['<start>']] * inp.shape[0], 1\n",
        "        )\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, dec_cell, _ = decoder(\n",
        "                dec_input, dec_hidden, dec_cell, enc_output\n",
        "            )\n",
        "\n",
        "            loss += loss_object(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "# =========================\n",
        "# 8. Training Loop\n",
        "# =========================\n",
        "EPOCHS = 300\n",
        "dataset = tf.data.Dataset.from_tensor_slices((eng_seq, spa_seq)).batch(2)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for inp, targ in dataset:\n",
        "        batch_loss = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss {total_loss.numpy():.4f}\")\n",
        "\n",
        "# =========================\n",
        "# 9. Translation Function\n",
        "# =========================\n",
        "def translate(sentence):\n",
        "    inputs = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    inputs = pad_sequences(inputs, maxlen=max_eng_len, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    enc_out, enc_hidden, enc_cell = encoder(inputs)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_cell = enc_cell\n",
        "\n",
        "    dec_input = tf.expand_dims([spa_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_spa_len):\n",
        "        predictions, dec_hidden, dec_cell, attention_weights = decoder(\n",
        "            dec_input, dec_hidden, dec_cell, enc_out\n",
        "        )\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        word = spa_tokenizer.index_word.get(predicted_id, '')\n",
        "\n",
        "        if word == '<end>':\n",
        "            break\n",
        "\n",
        "        result += word + \" \"\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "# =========================\n",
        "# 10. Test Translation\n",
        "# =========================\n",
        "print(\"\\nTranslations:\")\n",
        "print(\"hello ->\", translate(\"hello\"))\n",
        "print(\"thank you ->\", translate(\"thank you\"))\n",
        "print(\"good night ->\", translate(\"good night\"))"
      ],
      "metadata": {
        "id": "1H0sOmFGd0Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Use the following short poetry dataset to simulate poem generation with a\n",
        "pre-trained GPT model:\n",
        "\n",
        "[\"Roses are red, violets are blue,\",\n",
        "\"Sugar is sweet, and so are you.\",\n",
        "\"The moon glows bright in silent skies,\",\n",
        "\"A bird sings where the soft wind sighs.\"]\n",
        "\n",
        "Using this dataset as a reference for poetic structure and language, generate a new 2-4\n",
        "line poem using a pre-trained GPT model (such as GPT-2). You may simulate\n",
        "fine-tuning by prompting the model with similar poetic patterns.\n",
        "\n",
        "Include your code, the prompt used, and the generated poem in your answer."
      ],
      "metadata": {
        "id": "VtdQpdoyc5RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# Load GPT-2 model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Prompt\n",
        "prompt = \"\"\"\n",
        "Write a short 2-4 line poem in the style of the following lines:\n",
        "\n",
        "Roses are red, violets are blue,\n",
        "Sugar is sweet, and so are you.\n",
        "The moon glows bright in silent skies,\n",
        "A bird sings where the soft wind sighs.\n",
        "\n",
        "New poem:\n",
        "\"\"\"\n",
        "\n",
        "# Generate text\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_length=120,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.8,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "efec5766714545ca9ebc101abf7dfde9",
            "378f7e73b1734ea38b786aa81738f2cb",
            "7e45f86f7350420cb79798ff05aa5e30",
            "9b97a05ae973465187d2e596c8712a70",
            "c084c4fd57d74689b8e14c3a8a18f490",
            "38d53e1716fd41548ab75a42bd5e3f9d",
            "e8c76549952e40848708e6773281ef59",
            "e9d24ad85ac4497781ff8352f4cfce38",
            "439ea9664fe94a26aa3f69b543e30577",
            "8b1939949e0343cba4995b0f200d2def",
            "208a0cd5d1634edca2c44da097ca70da"
          ]
        },
        "id": "FA3g96_GeDdV",
        "outputId": "1125d684-4765-4c28-dd36-09b0f0494206"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efec5766714545ca9ebc101abf7dfde9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Passing `generation_config` together with generation-related arguments=({'top_k', 'num_return_sequences', 'temperature', 'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Write a short 2-4 line poem in the style of the following lines:\n",
            "\n",
            "Roses are red, violets are blue,\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "New poem:\n",
            "\n",
            "Climb aboard the starship,\n",
            "\n",
            "The moon is blue and red and white,\n",
            "\n",
            "There's a great deal to see.\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "New poem:\n",
            "\n",
            "Climb aboard the starship,\n",
            "\n",
            "The moon is green and blue,\n",
            "\n",
            "You're called to the moon,\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "New poem:\n",
            "\n",
            "Climb aboard the starship,\n",
            "\n",
            "The moon is pink and pink,\n",
            "\n",
            "You're called to the moon,\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "New poem:\n",
            "\n",
            "Climb aboard the starship,\n",
            "\n",
            "The moon is purple and blue,\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "New poem:\n",
            "\n",
            "Climb aboard the starship,\n",
            "\n",
            "The moon is green and blue,\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. Imagine you are building a creative writing assistant for a publishing\n",
        "company. The assistant should generate story plots and character descriptions using\n",
        "Generative AI. Describe how you would design the system, including model selection,\n",
        "training data, bias mitigation, and evaluation methods. Explain the real-world challenges\n",
        "you might face.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Designing a creative writing assistant for a publishing company involves balancing generative power with ethical safeguards to produce compelling, original story plots and character descriptions. The system would leverage large language models fine-tuned for narrative creativity, ensuring outputs align with publishing standards like coherence, diversity, and market appeal.\n",
        "\n",
        "**Model Selection :**\n",
        "I'd select a transformer-based large language model like GPT-4o or Llama 3.1 (405B parameters) as the core, due to their strong performance in zero-shot creative text generation from conversation history. For domain specificity, fine-tune a smaller variant like Mistral-7B-Instruct on creative writing tasks using LoRA (Low-Rank Adaptation) to minimize compute costs while preserving narrative fluency. Integrate retrieval-augmented generation (RAG) with a vector database (e.g., FAISS) storing genre-specific plot tropes and archetypes, allowing dynamic injection of inspirations without hallucination.\n",
        "\n",
        "**Training Data :**\n",
        "Curate a high-quality dataset of 100K+ public-domain stories, outlines, and character sheets from sources like Project Gutenberg, Wattpad archives (with permissions), and anonymized publishing slush piles. Augment with synthetic data generated via self-instruct methods: prompt base models to create diverse plots across genres (fantasy, romance, thriller). Include metadata tags for genre, tone, length, and diversity markers. Preprocess to filter low-quality text (e.g., via perplexity scores) and ensure 40% representation of underrepresented voices in authorship.\n",
        "\n",
        "**Bias Mitigation :**\n",
        "Embed fairness at every layer: during data curation, use tools like Perspective API to flag toxic or stereotypical content, then oversample diverse examples (e.g., non-Western settings, LGBTQ+ characters). In training, apply debiasing techniques like counterfactual data augmentation—rewriting biased prompts (e.g., \"strong male hero\" → gender-neutral variants). At inference, deploy a multi-stage filter: (1) bias classifiers scoring outputs for stereotypes, (2) human-in-the-loop prompts for edge cases, and (3) style transfer modules to enforce inclusivity. Regularly audit with metrics like WEAT (Word Embedding Association Test) on generated characters.\n",
        "\n",
        "**Evaluation Methods**\n",
        "Use a hybrid human-AI framework:\n",
        "\n",
        "- Automated: ROUGE/BERTScore for coherence against gold-standard plots; novelty via Self-BLEU (lower is more original); diversity with n-gram uniqueness across batches.\n",
        "\n",
        "- Human: Blind A/B tests with editors rating plots/characters on scales for engagement (1-5), originality, and commercial viability; inter-annotator agreement via Cohen's kappa.\n",
        "\n",
        "- Publisher-Specific: Track downstream metrics like acceptance rates into editing pipelines or reader surveys. Conduct iterative red-teaming: generate adversarial prompts to expose weaknesses.\n",
        "\n",
        "**Real-World Challenges :**\n",
        "\n",
        "Scalability demands high GPU costs for fine-tuning (~$10K+ per run), risking budget overruns; mitigate with cloud bursting or model distillation. Legal hurdles include IP lawsuits over training data resemblance—address via opt-out datasets and indemnity clauses. User adoption faces skepticism from writers fearing job loss; counter with positioning as a \"co-pilot\" via collaborative interfaces. Output inconsistency (e.g., repetitive plots) requires prompt engineering evolution, while ethical pushback on AI authorship demands transparent watermarking (e.g., SynthID). Finally, evolving regulations like EU AI Act classification as \"high-risk\" could mandate audits, delaying deployment."
      ],
      "metadata": {
        "id": "lixC8N-1eJa5"
      }
    }
  ]
}